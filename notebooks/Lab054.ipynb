{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "It's not possible to reference data or vairables across different languages in a notebook.  But you can you temporary objects to get around this limitation, creatively.\r\n",
        "\r\n",
        "This notebook also shows how to connect to a WASB account \r\n",
        "\r\n",
        "**Make sure you complete Lab0005 first.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "# Azure storage access info\r\n",
        "# we can use my read-only data lake\r\n",
        "# you should've been a connection to this in Lab005 already\r\n",
        "blob_account_name = 'davewdemoblobs' # replace with your blob name\r\n",
        "blob_container_name = 'nyctaxi-staging' # replace with your container name\r\n",
        "blob_relative_path = 'reference-data/payment_type_lookup.csv' # replace with your relative folder path\r\n",
        "linked_service_name = 'davesdemoblobs' # replace with your linked service name\r\n",
        "\r\n",
        "# this should work but is not.  Not sure why\r\n",
        "# blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
        "# for now, we can do this manually, but definitely not a best practice\r\n",
        "# get the token from the git repo, this can't be committed\r\n",
        "blob_sas_token = 'sv=...'\r\n",
        "\r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely\r\n",
        "\r\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "\r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
        "print('Remote blob path: ' + wasb_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:13:35.8842974Z",
              "execution_start_time": "2021-04-12T18:13:36.0154761Z",
              "execution_finish_time": "2021-04-12T18:13:38.0879317Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remote blob path: wasbs://nyctaxi-staging@davewdemoblobs.blob.core.windows.net/reference-data/payment_type_lookup.csv"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfPayTypes = spark.read.option(\"header\", \"true\") \\\r\n",
        "            .option(\"delimiter\",\"|\") \\\r\n",
        "            .csv(wasb_path)\r\n",
        "            #.schema(schema) \\"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:14:52.8789607Z",
              "execution_start_time": "2021-04-12T18:14:52.9878051Z",
              "execution_finish_time": "2021-04-12T18:14:55.0578691Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfPayTypes.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:15:02.3052592Z",
              "execution_start_time": "2021-04-12T18:15:02.4443936Z",
              "execution_finish_time": "2021-04-12T18:15:04.5131735Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+-----------+\n",
            "|payment_type|abbreviation|description|\n",
            "+------------+------------+-----------+\n",
            "|           1|        null|Credit card|\n",
            "|           2|        null|       Cash|\n",
            "|           3|        null|  No charge|\n",
            "|           4|        null|    Dispute|\n",
            "|           5|        null|    Unknown|\n",
            "|           6|        null|Voided trip|\n",
            "|           7|         CAS|       Cash|\n",
            "|           8|        CASH|       Cash|\n",
            "|           9|         CRD|Credit card|\n",
            "|          10|         CRE|Credit card|\n",
            "|          11|      CREDIT|Credit card|\n",
            "|          12|         CSH|       Cash|\n",
            "|          13|         Cas|       Cash|\n",
            "|          14|        Cash|       Cash|\n",
            "|          15|         Cre|Credit card|\n",
            "|          16|      Credit|Credit card|\n",
            "|          17|         DIS|    Dispute|\n",
            "|          18|         Dis|    Dispute|\n",
            "|          19|     Dispute|    Dispute|\n",
            "|          20|         NA |    Unknown|\n",
            "+------------+------------+-----------+\n",
            "only showing top 20 rows"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register df as a temp table\r\n",
        "# I like to keep the names the same to avoid confusion\r\n",
        "dfPayTypes.registerTempTable(\"dfPayTypes\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:15:35.5170545Z",
              "execution_start_time": "2021-04-12T18:15:35.6260268Z",
              "execution_finish_time": "2021-04-12T18:15:37.6821849Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "--now let's manipulate this table from SparkSQL\r\n",
        "select * from dfPayTypes"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:16:20.500553Z",
              "execution_start_time": "2021-04-12T18:16:20.6164103Z",
              "execution_finish_time": "2021-04-12T18:16:30.9603002Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "payment_type",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "abbreviation",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "description",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  }
                ]
              },
              "data": [
                [
                  "1",
                  null,
                  "Credit card"
                ],
                [
                  "2",
                  null,
                  "Cash"
                ],
                [
                  "3",
                  null,
                  "No charge"
                ],
                [
                  "4",
                  null,
                  "Dispute"
                ],
                [
                  "5",
                  null,
                  "Unknown"
                ],
                [
                  "6",
                  null,
                  "Voided trip"
                ],
                [
                  "7",
                  "CAS",
                  "Cash"
                ],
                [
                  "8",
                  "CASH",
                  "Cash"
                ],
                [
                  "9",
                  "CRD",
                  "Credit card"
                ],
                [
                  "10",
                  "CRE",
                  "Credit card"
                ],
                [
                  "11",
                  "CREDIT",
                  "Credit card"
                ],
                [
                  "12",
                  "CSH",
                  "Cash"
                ],
                [
                  "13",
                  "Cas",
                  "Cash"
                ],
                [
                  "14",
                  "Cash",
                  "Cash"
                ],
                [
                  "15",
                  "Cre",
                  "Credit card"
                ],
                [
                  "16",
                  "Credit",
                  "Credit card"
                ],
                [
                  "17",
                  "DIS",
                  "Dispute"
                ],
                [
                  "18",
                  "Dis",
                  "Dispute"
                ],
                [
                  "19",
                  "Dispute",
                  "Dispute"
                ],
                [
                  "20",
                  "NA ",
                  "Unknown"
                ],
                [
                  "21",
                  "NOC",
                  "No charge"
                ],
                [
                  "22",
                  "No ",
                  "No charge"
                ],
                [
                  "23",
                  "No Charge",
                  "No charge"
                ],
                [
                  "24",
                  "UNK",
                  "Unknown"
                ]
              ]
            },
            "text/plain": "<Spark SQL result set with 24 rows and 3 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "//that is the magic for scala.  \r\n",
        "//TODO:  need to figure out how to do this.  I can do it with an externally managed table,\r\n",
        "//this is proving difficult\r\n",
        "\r\n",
        "//let's look at the table from Scala\r\n",
        "//sqlContext.sql(\"select * from dfPayTypes\")\r\n",
        "//val dfPayTypes = spark.read.synapsesql(\"sqlpool.dbo.dfPayTypes\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 6,
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-04-12T18:27:09.3694785Z",
              "execution_start_time": "2021-04-12T18:27:09.4978513Z",
              "execution_finish_time": "2021-04-12T18:27:13.6407059Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 6, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "StructuredStream-spark package version: 2.4.5-1.3.1",
          "traceback": [
            "Error: StructuredStream-spark package version: 2.4.5-1.3.1",
            "com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '<token-identified principal>'. ClientConnectionId:2ea061bf-d85c-46a5-a27e-24433e63bc85\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262)\n",
            "  at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:283)\n",
            "  at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:129)\n",
            "  at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:37)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5233)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:3988)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:85)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:3932)\n",
            "  at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7375)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3206)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2713)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2362)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2213)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1276)\n",
            "  at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:861)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n",
            "  at com.microsoft.spark.sqlanalytics.utils.SQLAnalyticsJDBCWrapper$$anonfun$3.apply(SQLAnalyticsJDBCWrapper.scala:70)\n",
            "  at com.microsoft.spark.sqlanalytics.utils.SQLAnalyticsJDBCWrapper$$anonfun$3.apply(SQLAnalyticsJDBCWrapper.scala:70)\n",
            "  at scala.util.Try$.apply(Try.scala:192)\n",
            "  at com.microsoft.spark.sqlanalytics.utils.SQLAnalyticsJDBCWrapper.createConnection(SQLAnalyticsJDBCWrapper.scala:70)\n",
            "  at com.microsoft.spark.sqlanalytics.utils.Utils$.createConnection(Utils.scala:291)\n",
            "  at com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.com$microsoft$spark$sqlanalytics$read$SQLAnalyticsReader$$connection$lzycompute(SQLAnalyticsReader.scala:42)\n",
            "  at com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.com$microsoft$spark$sqlanalytics$read$SQLAnalyticsReader$$connection(SQLAnalyticsReader.scala:42)\n",
            "  at com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.readSchema(SQLAnalyticsReader.scala:84)\n",
            "  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:175)\n",
            "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:204)\n",
            "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n",
            "  at org.apache.spark.sql.SqlAnalyticsConnector$SQLAnalyticsFormatReader.sqlanalytics(SqlAnalyticsConnector.scala:42)\n",
            "  at org.apache.spark.sql.SqlAnalyticsConnector$SQLAnalyticsFormatReader.synapsesql(SqlAnalyticsConnector.scala:25)\n",
            "  ... 52 elided\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "//this is the code for scala to read a table from Serverless SQL Pool\r\n",
        "val scalaDataFrame = spark.read.sqlanalytics(\"mySQLPoolDatabase.dbo.mySQLPoolTable\")\r\n",
        "scalaDataFrame.createOrReplaceTempView( \"mydataframetable\" )\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "SELECT * FROM mydataframetable"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}