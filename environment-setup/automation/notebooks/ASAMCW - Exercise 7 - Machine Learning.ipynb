{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a classifier to determine product seasonality\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, check if XGBoost is properly installed in the Spark environment (should have version 1.0.2)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pip\n",
        "pip.get_installed_distributions()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all necessary libraries.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from onnxmltools.convert import convert_xgboost\n",
        "from onnxmltools.convert.common.data_types import FloatTensorType\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.webservice import Webservice"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory data analysis (basic stats)\n",
        "\n",
        "Create Spark temporary views for sales and products."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Seasonality"
            ],
            "values": [
              "ProductId"
            ],
            "yLabel": "ProductId",
            "xLabel": "Seasonality",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "ProductId": {
              "1": 913878,
              "2": 273395,
              "3": 267388
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "%%spark\n",
        "val df = spark.read.sqlanalytics(\"SQLPool01.wwi_mcw.SaleSmall\") \n",
        "df.createOrReplaceTempView(\"sale\")\n",
        "\n",
        "val df2 = spark.read.sqlanalytics(\"SQLPool01.wwi_mcw.Product\") \n",
        "df2.createOrReplaceTempView(\"product\")\n",
        "display(df2)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load daily product sales from the SQL pool.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "sqlQuery = \"\"\"\n",
        "SELECT\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "    ,COUNT(*) as TransactionItemsCount\n",
        "FROM\n",
        "    sale S\n",
        "    JOIN product P ON\n",
        "        S.ProductId = P.ProductId\n",
        "GROUP BY\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "\"\"\"\n",
        "\n",
        "prod_df = spark.sql(sqlQuery)\n",
        "prod_df.cache()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the number of records in the data frame (should be around 2.6 million rows)."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_df.count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display some statistics about the data frame.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "summary"
            ],
            "values": [
              "summary"
            ],
            "yLabel": "summary",
            "xLabel": "summary",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": {
            "summary": {
              "count": 1,
              "max": 1,
              "mean": 1,
              "min": 1,
              "stddev": 1
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(prod_df.describe())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pivot the data frame to make daily sale items counts columns. \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_df.groupBy(['ProductId', 'Seasonality']).pivot('TransactionDateId').sum('TransactionItemsCount').toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean up the nulls and take a look at the result.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_prep_df.fillna(0)\n",
        "prod_prep_df.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Isolate features and prediction classes.\n",
        "\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X = prod_prep_df.iloc[:, 2:].values\n",
        "y = prod_prep_df['Seasonality'].values\n",
        "\n",
        "X_scale = StandardScaler().fit_transform(X)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use PCA for dimensionality reduction\n",
        "\n",
        "Perform dimensionality reduction using Principal Components Analysis and two target components.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "pca_df = pd.DataFrame(data = principal_components, columns = ['pc1', 'pc2'])\n",
        "pca_df = pd.concat([pca_df, prod_prep_df[['Seasonality']]], axis = 1)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the products data frame in two dimensions (mapped to the two principal components).\n",
        "\n",
        "Note the clear separation of clusters.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize = (6,6))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [1, 2, 3]\n",
        "colors = ['r', 'g', 'b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = pca_df['Seasonality'] == target\n",
        "    ax.scatter(pca_df.loc[indicesToKeep, 'pc1']\n",
        "               , pca_df.loc[indicesToKeep, 'pc2']\n",
        "               , c = color\n",
        "               , s = 1)\n",
        "ax.legend(['All Season Products', 'Summer Products', 'Winter Products'])\n",
        "ax.plot([-0.05, 1.05], [0.77, 1.0], linestyle=':', linewidth=1, color='y')\n",
        "ax.plot([-0.05, 1.05], [0.37, 0.6], linestyle=':', linewidth=1, color='y')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo the Principal Components Analysis, this time with twenty dimensions.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def col_name(x):\n",
        "    return f'f{x:02}'\n",
        "\n",
        "pca = PCA(n_components=20)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "X = pd.DataFrame(data = principal_components, columns = list(map(col_name, np.arange(0, 20))))\n",
        "pca_df = pd.concat([X, prod_prep_df[['ProductId']]], axis = 1)\n",
        "pca_automl_df = pd.concat([X, prod_prep_df[['Seasonality']]], axis = 1)\n",
        "\n",
        "X = X[:4500]\n",
        "y = prod_prep_df['Seasonality'][:4500]\n",
        "pca_automl_df = pca_automl_df[:4500]"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the PCA components to the SQL pool (you may ignore any warnings).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca_sdf = spark.createDataFrame(pca_df)\n",
        "pca_sdf.createOrReplaceTempView(\"productpca\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "%%spark\n",
        "val df = spark.sqlContext.sql(\"select * from productpca\")\n",
        "df.write.sqlanalytics(\"SQLPool01.wwi_mcw.ProductPCA\", Constants.INTERNAL)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train ensemble of trees classifier (using XGBoost)\n",
        "\n",
        "Split into test and training data sets.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the ensemble classifier using XGBoost.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform predictions with the newly trained model.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the accuracy of the model using test data.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert trained model to ONNX format.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "initial_types = [\n",
        "    ('input', FloatTensorType([1, 20]))\n",
        "]\n",
        "\n",
        "onnx_model = convert_xgboost(model, initial_types=initial_types)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train classifier using Auto ML\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the connection to the Azure Machine Learning workspace. The Azure portal provides all the values below.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "subscription_id= '#SUBSCRIPTION_ID#'\n",
        "resource_group= '#RESOURCE_GROUP_NAME#'\n",
        "workspace_name= '#AML_WORKSPACE_NAME#'\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "ws.write_config()\n",
        "\n",
        "experiment = Experiment(ws, \"ASAMCW_Product_Seasonality\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the Automated Machine Learning experiment and start it (will run on local compute resources).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "automl_classifier_config = AutoMLConfig(\n",
        "        task='classification',        \n",
        "        experiment_timeout_minutes=15,\n",
        "        enable_onnx_compatible_models=True,\n",
        "        training_data=pca_automl_df,\n",
        "        label_column_name='Seasonality',\n",
        "        n_cross_validations=5,\n",
        "        enable_voting_ensemble=False,\n",
        "        enable_stack_ensemble=False\n",
        "        )\n",
        "\n",
        "local_run = experiment.submit(automl_classifier_config, show_output=True)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve and persist the best model\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "best_run, fitted_model = local_run.get_output()\n",
        "model_path = 'product_seasonality.pkl'\n",
        "joblib.dump(fitted_model, model_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operationalize\n",
        "Operationalization means getting the model into the cloud so that others can run it after you close the notebook. We will create a docker container running on Azure Container Instances (ACI) to host our model.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register the model\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model_name = \"ProductSeasonalityClassifier\"\n",
        "registered_model = Model.register(model_path = model_path, # this points to a local file\n",
        "                       model_name = model_name, # name the model is registered as\n",
        "                       tags = {'type': \"classification\"}, \n",
        "                       description = \"Product Seasonality Classifier\", \n",
        "                       workspace = ws)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Develop the scoring script\n",
        "For deployment, we need a function that will exercise the model with a sampling of data. This has been created for us and is available as part of the model's output artifacts.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "scoring_script = \"\"\"\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azureml.train.automl\n",
        "from sklearn.externals import joblib\n",
        "from azureml.core.model import Model\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # This name is model.id of model that we want to deploy deserialize the model file back\n",
        "    # into a sklearn model\n",
        "    model_path = Model.get_model_path(model_name = 'ProductSeasonalityClassifier')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "def run(input_json):     \n",
        "    try:\n",
        "        data_df = pd.read_json(input_json)       \n",
        "        # Get the predictions...\n",
        "        prediction = model.predict(data_df)\n",
        "        prediction = json.dumps(prediction.tolist())\n",
        "    except Exception as e:\n",
        "        prediction = str(e)\n",
        "    return prediction\n",
        "\"\"\"\n",
        "exec(scoring_script)\n",
        "with open(\"scoring_script.py\", \"w\") as file:\n",
        "    file.write(scoring_script)\n",
        "    \n",
        "scoring_script_file_name = 'scoring_script.py'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "#test locally\n",
        "json_test_data = X_test.to_json()\n",
        "init()\n",
        "run(json_test_data)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the model as a Web Service on Azure Container Instances (ACI)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# obtain conda dependencies from the automl run and save the file locally\n",
        "environment_config_file = 'conda_env.yml'\n",
        "best_run.download_file('outputs/conda_env_v_1_0_0.yml', environment_config_file)\n",
        "with open('conda_env.yml', 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "# create the environment based on the saved conda dependencies file\n",
        "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=environment_config_file)\n",
        "myenv.register(workspace=ws)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Configure and deploy the web service to Azure Container Instances\n",
        "inference_config = InferenceConfig(environment=myenv, entry_script=scoring_script_file_name)\n",
        "aci_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb= 2, tags = { 'type' : 'automl-classification'}, description='AutoML Product Seasonality Classifier Service')\n",
        "aci_service_name = 'automl-product-classifier-01'\n",
        "aci_service = Model.deploy(ws, aci_service_name, [registered_model], inference_config, aci_config)\n",
        "aci_service.wait_for_deployment(show_output = True)\n",
        "print(aci_service.state)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Call the Web Service\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "aci_service.run(json_test_data)"
      ],
      "attachments": {}
    }
  ]
}