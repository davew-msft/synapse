{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "1+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake and Other Topics\n",
        "\n",
        "Let's continue from last time we met.  \n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "Make sure you modify this as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# variables, setup, and imports\r\n",
        "\r\n",
        "# where can you write your data?  I call this a \"sandbox\"\r\n",
        "\r\n",
        "#CHANGEME\r\n",
        "whoami = mssparkutils.env.getUserName()\r\n",
        "print(whoami)\r\n",
        "sandboxRoot = \"abfss://sandbox@davewsynapsedl.dfs.core.windows.net\"\r\n",
        "\r\n",
        "delta_table_path = \"{0}/{1}/delta-testing\".format(sandboxRoot,whoami)\r\n",
        "print(delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Doublecheck:  is this the sandbox path you want to use?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's doublecheck that you can write out some sample data without issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "dfData = spark.range(0,5)\n",
        "display(dfData)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "(dfData\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .save(delta_table_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's go look at what was created in the datalake using the FileExplorer.  \n",
        "\n",
        "Or, we can do this directly from code\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "mssparkutils.fs.ls(delta_table_path) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You should see a series of `snappy.parquet` files.  \r\n",
        "\r\n",
        "Basically, at the start of every notebook you'll likely want to have a spot where you can write out \"temp\" data and we do that in the \"sandbox\".  \r\n",
        "\r\n",
        "It's difficult to remember those long `abfss://` paths.  \r\n",
        "\r\n",
        "## Using Mounts Might be easier\r\n",
        "\r\n",
        "Let's check out how to do that.  \r\n",
        "\r\n",
        "`/` should be mounted to your Primary datalake in your workspace.  \r\n",
        "\r\n",
        "This helps with moving code from \"dev\" to \"test\" to \"prod\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "mssparkutils.fs.ls(\"/\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This is the exact same thing, but using a `cell magic`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs ls /"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "What we really want to do is start to think about abstracting away the implementation details of the data lake and its structure.  Let's be honest...thinking about filepaths is way too much cognitive burden.  We just want to think about the data and solving business problems.  \r\n",
        "\r\n",
        "## Maybe we can create a pattern that will make our lives easier.  \r\n",
        "\r\n",
        "Over time you need access to multiple data lakes:\r\n",
        "* the corporate data lake\r\n",
        "* the marketing team's data lake\r\n",
        "* your sandbox\r\n",
        "\r\n",
        "And over time the locations of those data lakes will change **especially as your code is promoted from dev to test to prod**.  We need a better way.  \r\n",
        "\r\n",
        "One pattern is to have a simple folder structure that everyone understands and can reference easily.  We use `mounts`.  \r\n",
        "\r\n",
        "Here's the mount pattern we'll use:\r\n",
        "\r\n",
        "* `/` : we start here\r\n",
        "* `/lake`:  this will be the location we write out our datasets.  If our code ever moves from dev to test to prod or our datalake location changes in Azure, it's not a problem, **we just change the mount location**\r\n",
        "* `/bronze`:  we'll map this to MY datalake so we can read data.  \r\n",
        "\r\n",
        "So the pattern for the rest of the day will be:\r\n",
        "* read the data from `/bronze` and write the data to `/lake`\r\n",
        "\r\n",
        "Make sense?  \r\n",
        "\r\n",
        "## In the real world...\r\n",
        "\r\n",
        "* your \"administrator\" would do all the mounts for you.  This would be done automatically whenever your Spark pool fires up **or** as a `%%run notebook` call at the top of every one of your notebooks.  \r\n",
        "* always create your mounts under `/mnt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# let's doublecheck our variables\r\n",
        "print(sandboxRoot)\r\n",
        "print(whoami)\r\n",
        "\r\n",
        "lakepath = \"{0}/{1}/\".format(sandboxRoot,whoami)\r\n",
        "print(lakepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## we need to know the name of the Synapse \"LinkedService\" to the datalake.  \r\n",
        "## by doing this we want need to hardcode secrets/passwords in our code\r\n",
        "# CHANGEME\r\n",
        "LinkedService = \"davew-synapse-WorkspaceDefaultStorage\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# you might need to run this if you don't get it right the first time\r\n",
        "# or...at the end of your notebook to \"cleanup\"\r\n",
        "#mssparkutils.fs.unmount(\"/mnt/lake\") \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step one:  mount /lake to lakepath var.  This will be YOUR sandbox\r\n",
        "mssparkutils.fs.mount( \r\n",
        "    lakepath, \r\n",
        "    \"/mnt/lake\", \r\n",
        "    {\"LinkedService\":LinkedService} \r\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "mssparkutils.fs.ls(lakepath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Let's make sure that worked, notice it didn't.  Why?  \r\n",
        "mssparkutils.fs.ls(\"/\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# it's due to \"sessions\"\r\n",
        "# here's the syntax we should be using \r\n",
        "# mssparkutils.env.getJobId() gets our session\r\n",
        "# the path is really synfs:/{session}/mnt/lake\r\n",
        "synfsPath = \"synfs:/{0}/mnt/lake/\".format(mssparkutils.env.getJobId())\r\n",
        "print(synfsPath)\r\n",
        "mssparkutils.fs.ls(synfsPath) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# we should see the \"delta-testing folder\" we created earlier.  Let's do an ls on that\r\n",
        "mssparkutils.fs.ls(synfsPath + \"delta-testing\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Using mounts should be much easier.  \r\n",
        "\r\n",
        "You will need to develop your \"mount pattern\" at your company.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# now let's read that little df we built earlier\r\n",
        "\r\n",
        "dfReadFromSynFS = (spark\r\n",
        "    .read\r\n",
        "    .format(\"delta\")\r\n",
        "    .load(synfsPath + \"delta-testing\") )\r\n",
        "display(dfReadFromSynFS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Let's continue looking at Delta tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "columns = [\"CustomerID\", \"FirstName\", \"LastName\", \"Balance\"]\r\n",
        "vals = [\r\n",
        "     (1, \"Tre\", \"Cool\", 1887),\r\n",
        "     (2, \"Matt\", \"Cameron\", 1920),\r\n",
        "     (3, \"Taylor\", \"Hawkins\", 1892),\r\n",
        "     (4, \"Dave\", \"Grohl\", 1893),\r\n",
        "     (5, \"Jimmy\", \"Chamberlin\", 1901)\r\n",
        "]\r\n",
        "dfCustomers = spark.createDataFrame(vals, columns)\r\n",
        "dfCustomers.printSchema()\r\n",
        "display(dfCustomers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#let's save it to your sandbox\r\n",
        "(dfCustomers\r\n",
        "    .write\r\n",
        "    .format(\"delta\")\r\n",
        "    .mode(\"overwrite\")\r\n",
        "    .save(synfsPath + \"customers\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Remember, we can query a data lake directly from SQL\r\n",
        "\r\n",
        "BUT...we either need to get the path first, or \"marshall\" the python variable so SQL can use it.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(synfsPath)\r\n",
        "\r\n",
        "spark.conf.set(\"nv.synfsPath\", synfsPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "--you can use this syntax...\r\n",
        "--SELECT * FROM delta.`synfs:/0/mnt/lake/customers`;\r\n",
        "--or, this is probably better\r\n",
        "SELECT * FROM delta.`${nv.synfsPath}/customers`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updates\n",
        "\n",
        "You can update, delete, and merge (upsert) data into tables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "-- increase every balance by 10%\r\n",
        "UPDATE delta.`${nv.synfsPath}/customers` SET Balance = Balance * 1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "SELECT * FROM delta.`${nv.synfsPath}/customers`;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "DELETE FROM delta.`${nv.synfsPath}/customers` WHERE CustomerID = 3;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql \r\n",
        "\r\n",
        "SELECT * FROM delta.`${nv.synfsPath}/customers`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read older versions of data using Time Travel\r\n",
        "\r\n",
        "You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\r\n",
        "\r\n",
        "Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel).\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "DESCRIBE HISTORY delta.`${nv.synfsPath}/customers`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We can of course do something similar in pySpark.  Something like this:\r\n",
        "\r\n",
        "```spark\r\n",
        "delta_table.history().show(20, 1000, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Here are some other methods that might be interesting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "select * from delta.`${nv.synfsPath}customers` VERSION AS OF 1;\r\n",
        "select * from delta.`${nv.synfsPath}customers` VERSION AS OF 2;\r\n",
        "--you'll need to change the time accordingly\r\n",
        "select * from delta.`${nv.synfsPath}customers` TIMESTAMP AS OF '2023-05-10 15:48:07';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "--what do you think this code would do?  \r\n",
        "select * from delta.`${nv.synfsPath}customers@v1`\r\n",
        "EXCEPT ALL\r\n",
        "select * from delta.`${nv.synfsPath}customers@v2`\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's assume you are doing a data load and you've just built a new df that has all of the latest data and now you want to `UPSERT` that data into the existing `customers` delta file. \r\n",
        "\r\n",
        "Let's do that.  \r\n",
        "\r\n",
        "Let's assume `dfCustomersFromLatestDataLoad` already has the latest data staged and pre-processed.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE OR REPLACE TEMP VIEW dfCustomersFromLatestDataLoad \r\n",
        "AS \r\n",
        "SELECT 6 AS CustomerID, 'New' AS FirstName, 'Customer' AS LastName , 0 AS Balance\r\n",
        "    UNION ALL \r\n",
        "SELECT 5, 'JimmyJimmy', 'Chamberlin', 0\r\n",
        ";\r\n",
        "\r\n",
        "SELECT * FROM dfCustomersFromLatestDataLoad\r\n",
        ";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's UPSERT the data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "MERGE INTO delta.`${nv.synfsPath}customers` target\r\n",
        "USING dfCustomersFromLatestDataLoad source\r\n",
        "    ON source.CustomerID = target.CustomerID\r\n",
        "WHEN MATCHED THEN\r\n",
        "    UPDATE SET \r\n",
        "        target.FirstName = source.FirstName,\r\n",
        "        target.LastName = source.LastName\r\n",
        "WHEN NOT MATCHED THEN \r\n",
        "    INSERT (CustomerID, FirstName, LastName, Balance) VALUES (source.CustomerID, source.FirstName, source.LastName, source.Balance)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We can of course do all of this for pySpark using a python syntax, if desired.  "
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}