## Data Discovery with Spark

TODO:  use the exploration-spark.ipynb file instead of this

Discussion points:
* query-in-place vs data loading
* why use Spark and not just SQL Pools?

Let's look at how to do data discovery/sandboxing with Spark Pools.  

Navigate to `sale-small/Year=2018/Quarter=Q1/Month=1/Day=20180101/sale-small-20180101-snappy.parquet`, right click and choose **New notebook** then **Load to DataFrame**.

Let's add some additional cells and do some cool things:

```python
df.printSchema()
```

Now let's find the SUM and AVG Profit by day for each day in 2018 Q4, just like we did in the previous lab:

First we need to fix the existing cell:

```python
%%pyspark
df = spark.read.load('abfss://wwi-02@asadatalakedavew891.dfs.core.windows.net/sale-small/Year=2018/Quarter=Q4/*/*/*', format='parquet')
df.limit(10)
```

Now let's add a new cell

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

profitByDate = (df.groupBy("TransactionDate")
.agg(
    round(sum("ProfitAmount"),2).alias("(sum)Profit"),
    round(avg("ProfitAmount"),2).alias("(avg)Profit")
).orderBy("TransactionDate")
)
profitByDate.show(100)

```


Anytime you want a printed output to look pretty, wrap it in `display()`.  Try it by altering the last line above to:

`display(profitByDate.show(100))`

Note that now we can chart the data.  Try it.  

