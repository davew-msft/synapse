## Lab 058:  Loading Data from a Data Lake into Synapse SQL Pool using the "Integrate" box-and-line experience (ADF - Dataflows)

### Business Objective

Let's assume our `sales` data is already prepped in our `gold` area and we want to load it into `warehouse.factSales` as-is, in our EDW.  For now we only want to load `Year=2018` data.  

### Implementation

* Let's use the `Integrate` box-and-line tool to do this, using Dataflows.  

> **Note**: The current folder structure for daily sales data is as follows: 
    /wwi-02/sale-small/Year=`YYYY`/Quarter=`Q#`/Month=`M`/Day=`YYYYMMDD`, where `YYYY` is the 4 digit year (eg. 2019), `Q#` represents the quarter (eg. Q1), `M` represents the numerical month (eg. 1 for January) and finally `YYYYMMDD` represents a numeric date format representation (eg. `20190516` for May 16, 2019).
    > A single parquet file is stored each day folder with the name **sale-small-YYYYMMDD-snappy.parquet** (replacing `YYYYMMDD` with the numeric date representation).

```text
Sample path to the parquet folder for January 1, 2019:
/wwi-02/sale-small/Year=2019/Quarter=Q1/Month=1/Day=20190101/sale-small-20190101-snappy.parquet
```

## Lab

* Make sure your SQL Pool (the MPP EDW feature of Synapse) is running.  
  * DWU100 is adequate
* Please run [scripts/edw-tables.sql](./scripts/edw-tables.sql) against your SQL Pool to create the required objects, if you haven't done so already.  
* create 2 datasets 
  * one for the parquet files
  * one for the fact table
* `Publish All`
* Since we want to filter on a sale year folder(Year=2018) and copy only the 2018 sales data, we will need to create a data flow to define the specific data that we wish to retrieve from our source dataset. To create a new data flow, start by selecting **Develop** from the left menu, and in the **Develop** blade, expand the **+** button and select **Data flow**.
* For `Source` take all of the default settings except:
  * Wildcard Paths: `sale-small/Year=2018/\*/\*/\*/\*`
* For `Sink` take the defaults except:
  * toggle the **Auto mapping** setting to the off position. You will need to select Input columns manually since we are missing a few columns in the source
* `Publish All`
* We can now use this data flow as an activity in a pipeline. Create a new pipeline by selecting **Integrate** from the left menu, and in the **Integrate** blade, expand the **+** button and select **Pipeline**.
* From the **Activities** menu, expand the **Move & transform** section and drag an instance of **Data flow** to the design surface of the pipeline.
* In the **Adding data flow** blade, ensure **Use existing data flow** is selected
* Continue and complete the task.  
* Make sure you `Trigger` and test the dataflow.  
